{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform coins data using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code is run and saved on Colab. The output of this code is displayed in a CSV file in the same folder, called 'Coins_Cleaned.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jCQshu023uqE",
    "outputId": "db230c80-cb78-40e6-fad0-354c932982ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=eef528a8d154fda2eee4cc1ee84724e35029d8faf07a1802fe3dd3ec069c50d3\n",
      "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qr-8b-9s30cq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import explode, col, from_json, lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processes and combines cryptocurrency data using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TWfyMZiaCDQ2",
    "outputId": "d4600c96-193e-413e-bcb3-36a8cfcf6325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+-----------------+------------------+------------------+-----------------+------------------+------------------------+-----------------+--------------+------------------------+----+-------+------+---------+\n",
      "|circulating_supply|market_cap           |percent_change_1h|percent_change_24h|percent_change_30d|percent_change_7d|price             |timestamp               |total_supply     |volume_24h    |quote_timestamp         |id  |name   |symbol|is_active|\n",
      "+------------------+---------------------+-----------------+------------------+------------------+-----------------+------------------+------------------------+-----------------+--------------+------------------------+----+-------+------+---------+\n",
      "|3.55695351669E10  |2.274831167346767E10 |-0.044694212696  |8.922506415286    |1.344609752466    |-16.353234856636 |0.6395448117814203|2024-03-21T00:00:00.000Z|3.671463594304E10|9.7186707046E8|2024-03-21T00:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556951373739E10 |2.275209617360897E10 |0.016696656948   |7.018723559878    |2.382216307269    |-15.644473863828 |0.639651594384671 |2024-03-21T01:00:00.000Z|3.672871629896E10|9.5880150545E8|2024-03-21T01:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556951373739E10 |2.2765974420062412E10|0.06099766082    |8.583861041722    |1.998847743422    |-15.560266836576 |0.6400417668946441|2024-03-21T02:00:00.000Z|3.672871629896E10|9.4112320667E8|2024-03-21T02:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556950955862E10 |2.2819259366955654E10|0.23406697538    |7.398579046718    |1.41321655232     |-15.604281421478 |0.6415398932995853|2024-03-21T03:00:00.000Z|3.672871629896E10|9.347948967E8 |2024-03-21T03:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556950955862E10 |2.2664470033583115E10|-0.67832759549   |7.643743960284    |1.474936409986    |-16.034320254529 |0.6371881511672559|2024-03-21T04:00:00.000Z|3.672871629896E10|9.3443420264E8|2024-03-21T04:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556950912032E10 |2.2530701818109867E10|-0.590209864355  |8.314207655208    |0.714311221341    |-15.34243211625  |0.6334274038445648|2024-03-21T05:00:00.000Z|3.672871629896E10|9.1099414954E8|2024-03-21T05:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556950912032E10 |2.244611657319281E10 |-0.375422148852  |8.702214105437    |0.623278743916    |-16.476093554822 |0.6310493770736321|2024-03-21T06:00:00.000Z|3.672871629896E10|8.7349805007E8|2024-03-21T06:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556950958213E10 |2.244337176818807E10 |-0.012229716375  |6.801960723492    |-0.060730116665   |-16.99279264954  |0.6309722015246323|2024-03-21T07:00:00.000Z|3.672871629896E10|8.6645355997E8|2024-03-21T07:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556950958213E10 |2.249212685259953E10 |0.217236005869   |3.761090067898    |0.58706967072     |-19.473854154502 |0.6323429003333658|2024-03-21T08:00:00.000Z|3.672871629896E10|8.4192171122E8|2024-03-21T08:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556950346193E10 |2.2618636888531235E10|0.562480945418   |5.422314649215    |2.01452521546     |-19.522297682936 |0.635899708657444 |2024-03-21T09:00:00.000Z|3.672871629896E10|8.3468199502E8|2024-03-21T09:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556950346193E10 |2.249840106732832E10 |-0.531578546468  |4.671551941675    |1.744014545747    |-21.029956321249 |0.6325194022291689|2024-03-21T10:00:00.000Z|3.672871629896E10|8.2951258404E8|2024-03-21T10:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.55695058027E10  |2.2584148940291615E10|0.381122136108   |5.228677216389    |0.368751433959    |-19.995832176897 |0.6349300736862447|2024-03-21T11:00:00.000Z|3.672871629896E10|8.3061985459E8|2024-03-21T11:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.55695058027E10  |2.2497125136285362E10|-0.385331341182  |3.581797419175    |0.317260864625    |-20.436257056355 |0.6324834891177391|2024-03-21T12:00:00.000Z|3.672871629896E10|8.2293394887E8|2024-03-21T12:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556950572234E10 |2.2484212311078186E10|-0.057397442259  |3.259442311076    |0.930071552001    |-19.867167405083 |0.6321204597722767|2024-03-21T13:00:00.000Z|3.672871629896E10|8.1630507721E8|2024-03-21T13:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556950572234E10 |2.245638893512113E10 |-0.123746278376  |3.987493639251    |-0.955956942856   |-17.112330376057 |0.631338234228457 |2024-03-21T14:00:00.000Z|3.672871629896E10|8.1580091275E8|2024-03-21T14:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556950427727E10 |2.2571294808332134E10|0.511688635868   |3.947619088923    |1.960828329974    |-16.471728825418 |0.6345687202268906|2024-03-21T15:00:00.000Z|3.672871629896E10|8.1605282578E8|2024-03-21T15:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556950427727E10 |2.2573897136275146E10|0.011529369339   |5.866962177445    |4.066859950802    |-15.183241381551 |0.6346418819983544|2024-03-21T16:00:00.000Z|3.672871629896E10|8.0841929293E8|2024-03-21T16:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556950250021E10 |2.2880407856156826E10|1.357815450037   |9.033892310855    |6.47014056399     |-15.066885976773 |0.643259147524533 |2024-03-21T17:00:00.000Z|3.672871629896E10|7.7890052524E8|2024-03-21T17:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556950250021E10 |2.248281254384692E10 |-1.737710773381  |5.366415174737    |4.854777195331    |-16.228943591016 |0.6320811640172416|2024-03-21T18:00:00.000Z|3.672871629896E10|7.7507719925E8|2024-03-21T18:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "|3.556949792357E10 |2.2237887558009945E10|-1.089374935031  |0.454963164477    |3.618791179072    |-15.282623226681 |0.6251954302473887|2024-03-21T19:00:00.000Z|3.672871629896E10|7.7298511929E8|2024-03-21T19:00:00.000Z|2010|Cardano|ADA   |1        |\n",
      "+------------------+---------------------+-----------------+------------------+------------------+-----------------+------------------+------------------------+-----------------+--------------+------------------------+----+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, from_json, when, size\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Coin Price Analysis\").getOrCreate()\n",
    "\n",
    "# Define the path to the directory containing your JSON files\n",
    "path_to_json_directory = '/content/'\n",
    "\n",
    "# Loop over the dictionary keys, which represent the file indices.\n",
    "for file_index in range(1, 96):\n",
    "    json_file_path = f\"{path_to_json_directory}crypto{file_index}.json\"\n",
    "    rdd = spark.sparkContext.wholeTextFiles(json_file_path).values()\n",
    "    json_schema = spark.read.json(rdd).schema\n",
    "    df = spark.createDataFrame(rdd, StringType()) \\\n",
    "        .select(from_json(\"value\", json_schema).alias(\"data\")) \\\n",
    "        .select(\"data.*\")\n",
    "\n",
    "    # Loop over each currency within this file index.\n",
    "    for currency in df.select(\"data.*\").columns:\n",
    "        # Check if the currency is active before processing\n",
    "        df_exploded = df.select(\n",
    "    explode(col(f\"data.{currency}\")).alias(\"details\")\n",
    "    ).select(\n",
    "    col(\"details.quotes\"),\n",
    "    col(\"details.id\").alias(\"id\"),\n",
    "    col(\"details.name\").alias(\"name\"),\n",
    "    col(\"details.symbol\").alias(\"symbol\"),\n",
    "    col(\"details.is_active\").alias(\"is_active\")\n",
    "    )\n",
    "        df_currency = df_exploded.select(\"is_active\")\n",
    "        if df_currency.filter(col(\"is_active\") == 1).count() > 0:\n",
    "          df_final = df_exploded.select(\n",
    "        explode(col(\"quotes\")).alias(\"quotes\"), \"id\",\"name\",\"symbol\", \"is_active\").select(\n",
    "    col(\"quotes.quote.USD.*\"),\n",
    "    col(\"quotes.timestamp\").alias(\"quote_timestamp\"),\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"symbol\",\n",
    "    \"is_active\"\n",
    "    )\n",
    "\n",
    "        # Combine the DataFrame\n",
    "        if final_df is None:\n",
    "          final_df = df_final\n",
    "        else:\n",
    "          final_df = final_df.union(df_final)\n",
    "\n",
    "# Show the combined DataFrame\n",
    "final_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate moving average and add as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OreUFYAL3_F8"
   },
   "outputs": [],
   "source": [
    "# Moving average (MA)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Converts an ISO 8601 time string to a timestamp type\n",
    "df_sorted = final_df.withColumn(\"quote_timestamp\", F.to_timestamp(\"quote_timestamp\"))\n",
    "df_sorted = df_sorted.withColumn(\"unix_timestamp\", F.unix_timestamp(\"quote_timestamp\"))\n",
    "windowSpec = Window.partitionBy(\"id\") \\\n",
    "                   .orderBy(\"unix_timestamp\") \\\n",
    "                   .rowsBetween(-168, Window.currentRow)\n",
    "\n",
    "df_with_ma = df_sorted.withColumn(\"ma_7d\", F.avg(\"price\").over(windowSpec))\n",
    "df_with_ma.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the Spark DataFrame to a Pandas DataFrame and save it to a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uP5rfqmg4eGk"
   },
   "outputs": [],
   "source": [
    "pandas_df = df_with_ma.toPandas()\n",
    "pandas_df.to_pickle('/content/coins2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMz9C5rdgLNs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df0 = pd.read_pickle('/Users/eva/Downloads/coins0.pkl')\n",
    "df1 = pd.read_pickle('/Users/eva/Downloads/coins1.pkl')\n",
    "df2 = pd.read_pickle('/Users/eva/Downloads/coins2.pkl')\n",
    "\n",
    "# Concatenate them vertically\n",
    "combined_df = pd.concat([df0, df1], ignore_index=True)\n",
    "combined_df1 = pd.concat([combined_df, df2], ignore_index=True)\n",
    "\n",
    "combined_csv_file_path = '/Users/eva/Downloads/Coins_Cleaned.csv'\n",
    "combined_df1.to_csv(combined_csv_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
